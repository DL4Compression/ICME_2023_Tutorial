{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXE4qWxLPXCG",
        "outputId": "d12b1b5f-830f-4786-a48c-3fa2c915fa69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.7.0->torchmetrics) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "!pip install torchmetrics\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils import data\n",
        "from PIL import Image\n",
        "import os\n",
        "import wget\n",
        "import zipfile\n",
        "import torchvision\n",
        "from tqdm.notebook import tqdm as tq"
      ],
      "metadata": {
        "id": "3-tUhsNmPhJP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"stage1\": {\n",
        "        \"dest_path_data\": \"test_data/phase1.zip\",\n",
        "        \"url_data\": \"http://kliv.iitkgp.ac.in/projects/miriad/sample_data/bmi34/phase1/phase1.zip\",\n",
        "        \"url_model\": \"http://kliv.iitkgp.ac.in/projects/miriad/model_weights/bmi34/cbis_a1_b1.zip\",\n",
        "        \"dest_path_model\": \"model_weights/cbis_a1_b1.zip\"\n",
        "    },\n",
        "    \"test_data\": {\n",
        "        \"url\": \"http://kliv.iitkgp.ac.in/projects/miriad/sample_data/rbis_ddsm_sample.zip\",\n",
        "        \"dest_path\": \"test_data/rbis_ddsm_sample.zip\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "tjRajaS6PmCt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def download_and_extract(path, url, expath):\n",
        "    wget.download(url, path)\n",
        "    with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(expath)\n",
        "\n",
        "def download_data(config):\n",
        "    if not os.path.exists('test_data'):\n",
        "        os.makedirs('test_data')\n",
        "        data_url = config['stage1']['url_data']\n",
        "        data_path = config['stage1']['dest_path_data']\n",
        "        download_and_extract(path=data_path, url=data_url, expath='test_data/')\n",
        "\n",
        "# Here test_data coressponds to sample/dummy data to test the code.\n",
        "\n",
        "def download_model(config):\n",
        "    if not os.path.exists('model_weights'):\n",
        "        os.makedirs('model_weights')\n",
        "        data_url = config['stage1']['url_model']\n",
        "        data_path = config['stage1']['dest_path_model']\n",
        "        download_and_extract(path=data_path, url=data_url, expath='model_weights/')\n",
        "download_data(config)\n",
        "download_model(config)"
      ],
      "metadata": {
        "id": "-3K77mIxPszq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "    \"train\": {\n",
        "        \"train_data\": \"test_data/phase1/train/\",\n",
        "        \"test_data\": \"test_data/phase1/test/\",\n",
        "        \"batch_size\": 10,\n",
        "        \"lr\": 1e-8,\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "W6Zprq5UjJeE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDatasetPhase1(data.Dataset):\n",
        "\n",
        "    def __init__(self, path_to_dataset, files256=None,files128=None,split=None,\n",
        "                transform_images = None, transform_masks = None,\n",
        "                images_path_rel = '.', masks_path_rel = '.',\n",
        "                preserve_names = False):\n",
        "        self.path_to_dataset = os.path.abspath(path_to_dataset)\n",
        "        self.files256 = files256\n",
        "        self.files128 = files128\n",
        "        self.images_path_rel = images_path_rel # relative path to images\n",
        "        self.masks_path_rel = masks_path_rel # relative path to masks (same as images)\n",
        "        self.transform_images = transform_images # transforms\n",
        "        self.preserve_names = preserve_names # not important, debugging stuff\n",
        "        self.split = split\n",
        "        self.test_files = os.listdir(self.path_to_dataset)\n",
        "\n",
        "        # This is the list of all samples\n",
        "        self.cropimages = os.listdir(os.path.join(self.path_to_dataset, self.images_path_rel))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.split == 'train':\n",
        "            return min(len(self.files256),len(self.files128))\n",
        "        else:\n",
        "            return len(os.listdir(self.path_to_dataset))\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # indexing function\n",
        "\n",
        "        if self.split == 'train':\n",
        "            fname256 = self.files256[i]\n",
        "            fname128 = self.files128[i]\n",
        "            image256 = Image.open(os.path.join(self.path_to_dataset, fname256))\n",
        "            image128 = Image.open(os.path.join(self.path_to_dataset, fname128))\n",
        "            # mask = Image.open(os.path.join(self.path_to_dataset, self.masks_path_rel, self.cropsubset[i]))\n",
        "\n",
        "            # usual transformation apply\n",
        "            if self.transform_images is not None:\n",
        "                image256 = self.transform_images(image256)\n",
        "                image128 = self.transform_images(image128)\n",
        "\n",
        "            return [image256, image128]\n",
        "\n",
        "        else:\n",
        "            image = Image.open(os.path.join(self.path_to_dataset, self.test_files[i]))\n",
        "            if self.transform_images is not None:\n",
        "                image = self.transform_images(image)\n",
        "\n",
        "            return [image, 0, self.test_files[i]]"
      ],
      "metadata": {
        "id": "oixcLrjHjUu9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "images_transforms = torchvision.transforms.Compose([torchvision.transforms.Grayscale(),\n",
        "                                                    torchvision.transforms.ToTensor(),\n",
        "                                                    torchvision.transforms.Resize((1024,1024))])\n",
        "# Create data loaders.\n",
        "\n",
        "# CBISDDSM dataset & dataloader for inference\n",
        "test_dataset = CustomDatasetPhase1(model_config['train']['test_data'],\n",
        "                                    transform_images=images_transforms)\n",
        "test_dataloader = data.DataLoader(test_dataset,\n",
        "                                  batch_size=1, num_workers=4,\n",
        "                                  pin_memory=True, shuffle=False)\n",
        "print(len(test_dataloader))\n",
        "for X in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X[0].shape}\")\n",
        "    print(f\"Shape of X [N, C, H, W]: {X[1].shape}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKsC0PMdjW81",
        "outputId": "052f2bfa-58ec-4c2a-f512-92e853b2eb9f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([1, 1, 1024, 1024])\n",
            "Shape of X [N, C, H, W]: torch.Size([1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_downconv=3, n_encowidth=64):\n",
        "        super().__init__()\n",
        "        # a tunable number of DownConv blocks in the architecture\n",
        "        self.n_downconv = n_downconv\n",
        "        self.n_encowidth = n_encowidth\n",
        "        # The two mandatory initial layers\n",
        "        layer_list = [\n",
        "            nn.Conv2d(in_channels=1, out_channels=self.n_encowidth,\n",
        "                      kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=self.n_encowidth, out_channels=self.n_encowidth,\n",
        "                      kernel_size=3, stride=2, padding=1), nn.ReLU()\n",
        "        ]\n",
        "        for _ in range(self.n_downconv):\n",
        "            layer_list.extend([\n",
        "                nn.Conv2d(in_channels=self.n_encowidth, out_channels=self.n_encowidth,\n",
        "                          kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "                nn.Conv2d(in_channels=self.n_encowidth, out_channels=self.n_encowidth,\n",
        "                          kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
        "            ])\n",
        "        # The one mandatory end layer\n",
        "        layer_list.append(\n",
        "            nn.Conv2d(in_channels=self.n_encowidth, out_channels=16,\n",
        "                      kernel_size=3, stride=1, padding=1)\n",
        "        )\n",
        "        # register the Sequential module\n",
        "        self.encoder = nn.Sequential(*layer_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # forward pass; a final clamping is applied\n",
        "        return torch.clamp(self.encoder(x), 0, 1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_upconv=3, n_decowidth=96):\n",
        "        super().__init__()\n",
        "\n",
        "        # a tunable number of DownConv blocks in the architecture\n",
        "        self.n_upconv = n_upconv\n",
        "        self.n_decowidth = n_decowidth\n",
        "\n",
        "        # The one mandatory initial layers\n",
        "        layer_list = [\n",
        "            nn.Conv2d(in_channels=16, out_channels=n_decowidth,\n",
        "                      kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "        ]\n",
        "        # 'n_upconv' number of UpConv layers (In the CVPR paper, it was 3)\n",
        "        for _ in range(self.n_upconv):\n",
        "            layer_list.extend([\n",
        "                nn.Conv2d(in_channels=n_decowidth, out_channels=n_decowidth *\n",
        "                          4, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "                nn.PixelShuffle(2)\n",
        "            ])\n",
        "        # The mandatory final layer\n",
        "        layer_list.extend([\n",
        "            nn.Conv2d(in_channels=n_decowidth, out_channels=1 *\n",
        "                      4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.PixelShuffle(2)\n",
        "        ])\n",
        "        # register the Sequential module\n",
        "        self.decoder = nn.Sequential(*layer_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # forward pass; a final clamping is applied\n",
        "        return torch.clamp(self.decoder(x), 0, 1)\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, n_updownconv=3, width=64):\n",
        "        super().__init__()\n",
        "        self.n_updownconv = n_updownconv\n",
        "        self.width = width\n",
        "\n",
        "        # there must be same number of 'n_downconv' and 'n_upconv'\n",
        "        self.encoder = Encoder(\n",
        "            n_downconv=self.n_updownconv, n_encowidth=self.width)\n",
        "        self.decoder = Decoder(\n",
        "            n_upconv=self.n_updownconv, n_decowidth=self.width)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.shape_input = list(x.shape)\n",
        "        x = self.encoder(x)\n",
        "        self.shape_latent = list(x.shape)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "model = AutoEncoder(n_updownconv=3, width=64)\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s47sAbVZjhcg",
        "outputId": "8792090b-26f9-4119-d433-6050cb803906"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
        "from torchmetrics.image import PeakSignalNoiseRatio\n",
        "\n",
        "def compare_psnr_batch(original, compressed, **kwargs):\n",
        "\n",
        "    assert original.shape == compressed.shape, 'shapes should be same'\n",
        "    assert len(original.shape) == 4  # Batch x Channel x Height x Width\n",
        "\n",
        "    psnr = PeakSignalNoiseRatio().cuda()\n",
        "    avg_psnr = psnr(compressed, original)\n",
        "\n",
        "    return avg_psnr\n",
        "\n",
        "def compare_ssim_batch(original, compressed, **kwargs):\n",
        "    assert original.shape == compressed.shape # 'shapes should be same'\n",
        "    assert len(original.shape) == 4  # Batch x Channel x Height x Width\n",
        "\n",
        "    ssim = StructuralSimilarityIndexMeasure().cuda()\n",
        "    avg_ssim = ssim(original, compressed)\n",
        "\n",
        "    return avg_ssim"
      ],
      "metadata": {
        "id": "pA3j5sQyjpAE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model_phase1(config, test_dataloader, model):\n",
        "    n, avg_loss, avg_ssim, avg_psnr = 1, 0, 0, 0\n",
        "    for _, data_list in enumerate(test_dataloader):\n",
        "        images = data_list[0]\n",
        "        if torch.cuda.is_available():\n",
        "            images = images.cuda()\n",
        "\n",
        "        output = model(images)\n",
        "        # calculate the metrics (SSIM and pSNR)\n",
        "        ssim = compare_ssim_batch(images, output)\n",
        "        psnr = compare_psnr_batch(images, output)\n",
        "\n",
        "        avg_ssim = ((n * avg_ssim) + ssim) / (n + 1)  # running mean\n",
        "        avg_psnr = ((n * avg_psnr) + psnr) / (n + 1)  # running mean\n",
        "        n += 1\n",
        "    return avg_ssim, avg_psnr"
      ],
      "metadata": {
        "id": "W0_bbqa8jwFO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoEncoder(n_updownconv=3, width=64).cuda()\n",
        "saved_dict = torch.load(\"model_weights/cbis_a1_b1.model\")\n",
        "model.load_state_dict(saved_dict['model_state'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09-ExrJ8jy_O",
        "outputId": "ed6de373-2f74-493f-ad23-6828eaf93789"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_psnr,avg_ssim = validate_model_phase1(config, test_dataloader, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xextOMh3kFTP",
        "outputId": "c31e2973-8bc9-4138-fc6b-737896f0cac3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l7Qn5sBgm7XJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}